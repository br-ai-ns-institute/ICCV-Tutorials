<!DOCTYPE html>
<html>
<head>
<title style="font-family:Calibri;">ICCV Tutorials</title>
<head>
<body>
<div style="margin-left:5%; margin-right:5%;">
<h1 style= "margin-bottom:75px;text-align:center;font-family:Calibri;">ICCV Tutorials</h1>

<h2 style="color:blue;font-family:Calibri;">1. Towards Robust, Trustworthy, and Explainable Computer Vision</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">This tutorial will introduce participants to different aspects of computer vision models beyond performance. Ramprasaath R. Selvaraju will focus on explainable-AI methodologies and how understanding the decision process helps fixing various characteristics of the model. Sara Hooker will address the trustworthiness and the social impact of vision models. Bolei Zhou will focus on the interactive aspect of dissected vision models and its implication to visual editing applications. Aleksander Madry will focus on the robustness of vision models. Therefore, in this tutorial there will be a unification of different perspectives beyond test-set performance that are just as important to have in vision models.</p>
<a href="pdf/1/Towards Robust, Trustworthy, and Explainable Computer Vision.pdf" target="_blank">
<img src="icon/pdf.png" style="width:50px">
</a>

<a href="https://explainablevision.github.io/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:40px">
</a>


<h2 style="color:blue;font-family:Calibri;">2. Uncertainty Estimation for Dense Prediction Tasks</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Deep neural network can be roughly divided into deterministic neural network and stochastic neural network. The former is usually trained to achieve mapping from input space to the output space via maximum likelihood estimation for the weights, which leads to deterministic predictions during testing. In this way, a specific weights set is estimated while ignoring any uncertainty that may have in the proper weight space. The latter introduces randomness into the framework, either by assuming a prior distribution over model parameters (i.e. Bayesian Neural Networks) or including latent variables (i.e. generative models), leading to stochastic predictions during testing. Different from former that achieves point estimation, the latter aims to estimate prediction distribution, making it possible to estimate uncertainty representing model ignorance about it's predictions. In this tutorial, we will briefly introduce uncertainty estimation, including ensemble based solutions and generative model based solutions, and explain their pros and cons while using them in dense predictions tasks.</p>
<a href="pdf/2/Uncertainty Estimation for Dense Prediction Tasks.pdf" target="_blank">
<img src="icon/pdf.png" style="width:50px">
</a>

<a href="https://jingzhang617.github.io/uncertaintyestimation_iccv21.github.io/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:40px">
</a>


<h2 style="color:blue;font-family:Calibri;">3. Tutorial on Cross-Model Compatibility in Computer Vision</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Upon completion, participant will understand the basics on the problem of cross model compatibility and will be able to get a broad of view of this newly emerged research area. A participant will be able to understand of the major research challenges and application of cross model compatibility. The participant will be able to assess the use cases that would require cross model compatibility, for example, in vision model updates or multiple platform model deployment. The participant will also be able to identify the cases where the current solutions do not suffice.</p>
<a href="https://compatible-ml.org/iccv2021/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">4. Multi-Modality Learning from Videos and Beyond</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">In this tutorial, we would like to cover many aspects of multi-modality learning in vision, e.g., using language, audio, video, wireless, touch, etc. Our target audience includes students, researchers and engineers, who are interested in learning the recent advances in multi-modality, performing research and applying them to real-world problems.</p>
<a href="https://bryanyzhu.github.io/mm-iccv2021/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">5. Large-Scale Fine-Grained Food AnalysIs</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Upon completion, participant will be able to give the presentation, ask questions and answer the questions</p>
<a href="https://foodai-workshop.meituan.com/foodai2021.html#index" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">6. Introduction to Event Detection Camera</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Event detection cameras that emerged out of biologically inspired visual perception offer a rich area of engineering innovation, research, and applications. Event-based vision sensors (EVS) generates a sparse asynchronous data stream reporting temporal log-intensity changes (or events) of the pixel-sized photodiodes. EVS enjoys a far wider dynamic range (>100dB) and temporal resolution (>800kHz) compared to the familiar active pixel sensors (APS), while lacking the notion of pixel intensity. Because conventional intensity-based image processing and computer vision techniques designed for APS will fail for EVS, a new set of EVS-specific tools need to be developed. We will characterize operating characteristics of EVS, and establish details needed for researchers to begin working with event data. The goal of this proposed tutorial is to help the practitioners understand the key benefits and the characteristics of event detection cameras, establish a working knowledge of how event detection sensor data is processed, and develop a starting point for working with event data in their work environments. We cover common computational technique to work with sparse event data, from foundation to examples of state-of-the-art applications of event cameras. Event detection cameras are relatively new, and while the technology has gained the attention of many, tutorial resources are still limited. The technology is significantly different from conventional cameras and the “hurdle” for entry for working with event cameras are relatively high. Thus this tutorial is designed to bridge the knowledge gap of practitioners who already work in image processing and computer vision by providing a solid foundation for working with event data. We cover topics comprehensively (e.g. sensor architecture, data acquisition, signal processing, feature extraction, machine learning, applications), yet also have specific modules aimed at providing tools for tutorial participants to start working with event cameras.</p>
<a href="pdf/6/Introduction to Event Detection Camera.pdf" target="_blank">
<img src="icon/pdf.png" style="width:50px">
</a>
<a href="https://sites.google.com/a/udayton.edu/issl/cvip-colloquium/event-cameras" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:40px">
</a>


<h2 style="color:blue;font-family:Calibri;">7. 2nd Large Scale Holistic Video Understanding ICCV 2021 Tutorial</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Tutorial on Video Understanding. In specific, you will learn about: Holistic video recognition Future prediction in videos Large scale video understanding Multiple category recognition in videos Object and activity detection in videos Weakly supervised learning from web videos Learning visual representation from videos Unsupervised and self-supervised learning with videos 3D/2D Deep Neural Networks for action and activity recognition. In the last years, we have seen a tremendous progress in the capabilities of computer systems to classify video clips taken from the Internet or to analyze human actions in videos. There are lots of works in video recognition field focusing on specific video understanding tasks, such as, action recognition, scene understanding, etc. There have been great achievements in such tasks, however, there has not been enough attention toward the holistic video understanding task as a problem to be tackled. Current systems are expert in some specific fields of the general video understanding problem. However, for real world applications, such as, analyzing multiple concepts of a video for video search engines and media monitoring systems or providing an appropriate definition of the surrounding environment of an humanoid robot, a combination of current state-of-the-art methods should be used. Therefore in this tutorial, we intend to put effort into introducing the holistic video understanding as a new challenge in the computer vision field. This challenge focuses on the recognition of scenes, objects, actions, attributes, and events in the real world and user-generated videos. We also aim to cover the most important aspects of video recognition and understanding in the tutorial course work.</p>
<a href="https://holistic-video-understanding.github.io/tutorials/iccv2021.html" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">8. Benchmarking City-Scale Semantic 3D Map Making with Mapillary Metropolis</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Next-generation, location-based computer vision (CV) applications like augmented reality or autonomous driving require robustly working CV algorithms. Robustness means that algorithms can cope with variability in input data like seasonal and weather-related appearance changes, low-quality data from cheap cameras, or data captured under suboptimal lighting conditions. Producing reliable predictions in such challenging data scenarios and for highly-varying, city-scale environments makes a real impact for downstream applications.
To make this impact measurable, we are introducing a new, publicly accessible city-scale dataset called Mapillary Metropolis. This dataset is designed with the goal of creating a completely novel and complex benchmarking paradigm for training and testing computer vision algorithms in the context of semantic 3D map making. Our new dataset comprises multiple data modalities at a city-scale size, registered across different representations, and enriched with human- and machine-generated annotations for different object recognition and tracking tasks. These modalities include professional- and consumer-grade street-level images, aerial images, 3D point clouds from street-level LiDAR, aerial LiDAR, and image-based reconstruction (SfM and MVS), and CAD models. All data modalities are aligned based on manual correspondence annotations and ingestion of survey-grade ground control point data. Our dataset is designed to take city-scale 3D semantic modeling to the next level by enabling researchers to study shortcomings in current methods including, but not limited to object recognition and tracking, 3D modeling, depth estimation, relocalisation, image retrieval, change detection, sensor-fusion, etc. Upon completion, participant will be able to understand the scope, utilize, and experiment with different modalities in the novel Mapillary Metropolis dataset.</p>
<a href="https://research.mapillary.com/MetropolisTutorial" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">9. Building Digital Twins for Large Scale Augmented Reality</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">Upon completion, participants will be able to learn the cutting-edge techniques, such as object and scene understanding, world reconstruction and localization in the Augmented Reality (AR). Meanwhile, participants will discuss with academic and industiral leaders on the progress and the future of AR and VR.</p>
<a href="https://sites.google.com/view/perceptionatmagicleap/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">10. Theory and Application of Energy-Based Generative Models</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">In recent years, there has been growing interest in ConvNet-parametrized energy-based generative models. The concomitant need for representation, generation, efficiency and scalability in generative models is addressed by the framework of ConvNet-parametrized EBMs. Specifically, different from existing popular generative models, such as Generative Adversarial Nets (GANs) and Variational Auto-encoders (VAEs), the energy-based generative model can unify the bottom-up representation and top-down generation into a single framework, and can be trained by "analysis by synthesis", without recruiting an extra auxiliary model. Both model parameter update and data synthesis can be efficiently computed by back-propagation. The model can be easily designed and scaled up. The expressive power and advantages of this framework has launched a series of research works leading to significant theoretical and algorithmic maturity. Due to its major advantages over conventional models, energy-based generative models are now utilized in many computer vision tasks. The tutorial will provide a comprehensive introduction to energy-based generative modeling and learning in computer vision. An intuitive and systematic understanding of the underlying learning objective and sampling strategy will be developed. Different types of computer vision tasks successfully solved by the energy-based generative frameworks will be presented. Besides introducing the energy-based framework and the state-of-the-art applications, this tutorial will aim to enable researchers to apply the energy-based learning principles in other contexts of computer vision.</p>
<a href="https://energy-based-models.github.io/iccv2021-tutorial" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">11. Reviewing the Review Process</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">The goal of this tutorial is to provide the community with insights on the reviewing process from the perspective of experiences area and program chairs and to trigger discussions on how we can improve the process. Over the last years, the field of Computer Vision has experienced a tremendous growth. This is evident in the ever-growing number of participants of the main Computer Vision conferences, as well as industrial and public interest in this area of research. As part of this growth, the number of paper submissions is increasing at a rapid pace. Unfortunately, the number of experienced reviewers and area chairs is not increasing at the same rate. This could be linked to the fact that many senior researchers are nowadays (partially) affiliated with industry, and thus do not have the time to be part of the reviewing process in senior roles and to educate their PhD students about writing reviews. Another reason is the ever-increasing range of topics presented at CVPR/ECCV/ICCV, which makes it harder and harder to find experts for each submission. As a result, the quality of the reviews seems to be decreasing, leading to more random decisions and thus frustration in the community. It is left to us as a research community to make an effort to actively work on increasing the quality of the review process. This tutorial is a follow-up to a successful CVPR 2020 tutorial (the video recording of the tutorial has been watched more than 15k times on YouTube). Last year’s tutorial had a broad focus, covering the process from writing a paper, going over writing reviews and rebuttals, and finally, to how reviews are used by Area Chairs (AC). Based on questions from the audience, this year’s edition focuses on the latter parts of the process: From an AC’s perspective: what is useful in a review and in a rebuttal? What are the types of reviews and discussions that are helpful in the decision process? From a program chair’s (PC) perspective: how can we improve the review and the decision process? How can we educate our authors, reviewers, and area chairs? How is the process currently structured and where are the current bottlenecks? In order to achieve these goals, we plan to provide diverse perspectives from both relatively young and well-established researchers, area chairs and program chairs from recent conferences. We believe that by providing multiple perspectives on the topic, attendees will be able to better understand the review process and, as a consequence, help us as a community to improve its quality. We hope that by educating the community, we will make the process more transparent, thus increasing the trust in the system.</p>
<a href="https://sites.google.com/view/reviewing-the-review-process/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">12. Large-Scale Visual Localization</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">The goal of this tutorial is to teach the audience basic and advanced concepts of visual localization. This includes place recognition via image retrieval, camera pose estimation, feature-based visual localization from 2D-3D matches, privacy-preserving localization algorithms, and learning-based techniques. The tutorial covers the task of visual localization at large-scale, where the goal is to localize a single image solely based on visual information. The tutorial includes localization approaches for different granularity levels, ranging from simple recognition of named locations and GPS estimation to the precise estimation of the 6D camera pose. The tutorial’s scope covers cases with different spatial/geographical extend, e.g. a small indoor/outdoor scene, city-level, world-level, as well as localization under changing conditions.</p>
<a href="https://sites.google.com/view/lsvpr2021/home" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


<h2 style="color:blue;font-family:Calibri;">13. Efficient Video Understanding: State-of-the-art, Challenges, and Opportunities</h2>
<h2 style="font-family:Calibri;"> Learning Objectives:</h2>
<p style="font-family:Calibri;text-align:justify;">The objective of this tutorial is to present the audience with a unifying perspective of efficient video understanding from both theoretical and application standpoint, as well as to discuss state-of-the-art, challenges, motivate and encourage future research and opportunities that will spur disruptive progress in the the emerging field of efficient computer vision for democratizing AI technology. Widespread visual sensors and unprecedented connectivity have left us awash with video data -- ranging from consumer home videos to enterprise video data across many different industries, including media and entertainment, healthcare, and safety/security. Much progress has been made in computer vision to automatically understand this complex video content. However, despite impressive results on commonly used benchmark datasets, efficiency remains a great challenge due to the high computation, memory, energy and labeled data requirement of deep video understanding models. This poses an issue for deploying these models in many resource-limited applications such as autonomous vehicles and mobile platforms, and domains where fast inference is essential, such as video analysis for media and entertainment. Motivated by the need of efficiency, extensive studies have been recently conducted in computer vision that focus on designing compact models for computation efficiency, learning from unlabeled videos for data or label efficiency and neural architecture search for design efficiency. Moreover, new important research topics and problems are also recently appearing, (1) efficient multi-modal learning, (2) dynamic neural networks, (3) self-supervised learning for videos, (4) design of specialized hardware, (5) automatic design of architectures, and (6) energy efficient computing for green AI. While these recent works and problems are opening up new paths forward, our understanding on different aspects of efficiency in video understanding remains far from complete. The goal of this tutorial is to discuss state-of-the-art, challenges, motivate and encourage future research and opportunities that will spur disruptive progress in the the emerging field of efficient computer vision for democratizing AI technology.</p>
<a href="https://sites.google.com/view/effvideo-2021/" target="_blank">
<img src="icon/webpage.png" style="width:60px; margin-left:0px">
</a>


</div>
</body>